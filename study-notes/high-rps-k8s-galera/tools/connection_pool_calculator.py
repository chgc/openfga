#!/usr/bin/env python3
"""
OpenFGA + MariaDB Galera é€£æ¥æ± é…ç½®è¨ˆç®—å™¨
ç”¨æ–¼è¨ˆç®—åœ¨ç‰¹å®š RPS å’Œè³‡æºé™åˆ¶ä¸‹çš„æœ€å„ªé€£æ¥æ± é…ç½®
"""

import math
from dataclasses import dataclass
from typing import Dict, List, Tuple


@dataclass
class RPSScenario:
    """RPS å ´æ™¯åƒæ•¸"""
    target_rps: int
    avg_latency_ms: int  # å¹³å‡æŸ¥è©¢å»¶é²ï¼ˆæ¯«ç§’ï¼‰
    safety_factor: float  # å®‰å…¨ä¿‚æ•¸ï¼ˆ1.2-2.0ï¼‰
    pod_replicas: int  # Pod å‰¯æœ¬æ•¸


@dataclass
class DatabaseConfig:
    """è³‡æ–™åº«é…ç½®"""
    max_open_conns: int
    max_idle_conns: int
    conn_max_idle_time: str
    conn_max_lifetime: str
    min_connections_per_node: int  # æ¯å€‹ Galera ç¯€é»çš„æœ€å°é€£æ¥


def calculate_required_connections(scenario: RPSScenario) -> int:
    """
    è¨ˆç®—é”åˆ°ç›®æ¨™ RPS éœ€è¦çš„ç¸½é€£æ¥æ•¸
    
    å…¬å¼: éœ€è¦é€£æ¥æ•¸ = (RPS Ã— å¹³å‡å»¶é² / 1000) Ã— å®‰å…¨ä¿‚æ•¸
    """
    total_connections = (
        scenario.target_rps 
        * scenario.avg_latency_ms 
        / 1000 
        * scenario.safety_factor
    )
    return math.ceil(total_connections)


def calculate_per_pod_config(
    total_required_conns: int,
    pod_replicas: int,
    galera_nodes: int = 3
) -> Tuple[int, int]:
    """
    è¨ˆç®—æ¯å€‹ Pod çš„é€£æ¥æ± é…ç½®
    
    è¿”å›: (MaxOpenConns, MaxIdleConns)
    """
    # æ¯å€‹ Pod çš„æœ€å¤§é–‹æ”¾é€£æ¥æ•¸
    max_open_per_pod = math.ceil(total_required_conns / pod_replicas)
    
    # æœ€å¤§ç©ºé–’é€£æ¥æ•¸ï¼ˆå»ºè­°ç‚º MaxOpenConns çš„ 30-50%ï¼‰
    max_idle_per_pod = math.ceil(max_open_per_pod * 0.4)
    
    return max_open_per_pod, max_idle_per_pod


def calculate_galera_max_connections(
    total_open_conns: int,
    galera_nodes: int = 3,
    buffer_percentage: float = 0.2
) -> int:
    """
    è¨ˆç®— MariaDB Galera max_connections è¨­ç½®
    
    è€ƒæ…®:
    - OpenFGA é€£æ¥
    - å…§éƒ¨ Galera é€šä¿¡é€£æ¥
    - å‚™ç”¨ç·©è¡ï¼ˆ20%ï¼‰
    """
    internal_connections = galera_nodes * 5  # æ¯å€‹ç¯€é»çš„å…§éƒ¨é€£æ¥
    buffer = math.ceil(total_open_conns * buffer_percentage)
    
    max_connections = total_open_conns + internal_connections + buffer
    return max(2000, max_connections)  # è‡³å°‘ 2000


def recommend_idle_and_lifetime() -> Dict[str, str]:
    """
    æ ¹æ“šå ´æ™¯æ¨è–¦ ConnMaxIdleTime å’Œ ConnMaxLifetime
    """
    return {
        "ConnMaxIdleTime": "60s",      # 60 ç§’å¾Œå›æ”¶ç©ºé–’é€£æ¥
        "ConnMaxLifetime": "10m",      # 10 åˆ†é˜å¾Œå¼·åˆ¶æ›´æ–°é€£æ¥
    }


def calculate_cpu_memory_resources(
    total_rps: int,
    avg_latency_ms: int,
    pod_replicas: int
) -> Dict[str, Dict[str, str]]:
    """
    æ ¹æ“š RPS å’Œå»¶é²è¨ˆç®— CPU å’Œè¨˜æ†¶é«”è³‡æºéœ€æ±‚
    """
    # ç°¡åŒ–æ¨¡å‹ï¼šæ¯ 1000 RPS éœ€è¦ 500m CPU
    rps_per_pod = total_rps / pod_replicas
    
    cpu_request_m = math.ceil((rps_per_pod / 1000) * 500)
    cpu_limit_m = cpu_request_m * 4  # é™åˆ¶ç‚ºè«‹æ±‚çš„ 4 å€
    
    # è¨˜æ†¶é«”åŸºæº– + é€£æ¥é–‹éŠ·ï¼ˆæ¯å€‹é€£æ¥ ~1-2MBï¼‰
    base_memory_mi = 256
    total_conns = calculate_required_connections(
        RPSScenario(
            target_rps=total_rps,
            avg_latency_ms=avg_latency_ms,
            safety_factor=1.5,
            pod_replicas=pod_replicas
        )
    )
    connection_memory = total_conns / pod_replicas / 1000 * 500  # ç²—ç•¥ä¼°ç®—
    
    memory_request_mi = base_memory_mi + int(connection_memory)
    memory_limit_mi = memory_request_mi * 4
    
    return {
        "openfga": {
            "cpu_request": f"{cpu_request_m}m",
            "cpu_limit": f"{cpu_limit_m}m",
            "memory_request": f"{memory_request_mi}Mi",
            "memory_limit": f"{memory_limit_mi}Mi",
        },
        "mariadb": {
            "cpu_request": "1000m",
            "cpu_limit": "4000m",
            "memory_request": "2Gi",
            "memory_limit": "4Gi",
        }
    }


def print_recommendation(
    scenario: RPSScenario,
    galera_nodes: int = 3
) -> None:
    """
    æ‰“å°å®Œæ•´çš„é…ç½®å»ºè­°
    """
    # è¨ˆç®—éœ€è¦çš„é€£æ¥æ•¸
    total_conns = calculate_required_connections(scenario)
    
    # è¨ˆç®—æ¯å€‹ Pod çš„é…ç½®
    max_open, max_idle = calculate_per_pod_config(
        total_conns,
        scenario.pod_replicas,
        galera_nodes
    )
    
    # è¨ˆç®— Galera é…ç½®
    galera_max_conns = calculate_galera_max_connections(
        scenario.target_rps * scenario.safety_factor / 1000 * scenario.avg_latency_ms,
        galera_nodes
    )
    
    # è¨ˆç®—è³‡æº
    resources = calculate_cpu_memory_resources(
        scenario.target_rps,
        scenario.avg_latency_ms,
        scenario.pod_replicas
    )
    
    print("\n" + "="*80)
    print("OpenFGA + MariaDB Galera é€£æ¥æ± é…ç½®å»ºè­°")
    print("="*80)
    
    print(f"\nğŸ“Š å ´æ™¯åƒæ•¸:")
    print(f"  â€¢ ç›®æ¨™ RPS: {scenario.target_rps:,}")
    print(f"  â€¢ å¹³å‡å»¶é²: {scenario.avg_latency_ms}ms")
    print(f"  â€¢ å®‰å…¨ä¿‚æ•¸: {scenario.safety_factor}")
    print(f"  â€¢ Pod å‰¯æœ¬: {scenario.pod_replicas}")
    print(f"  â€¢ Galera ç¯€é»: {galera_nodes}")
    
    print(f"\nğŸ”Œ é€£æ¥æ± é…ç½®:")
    print(f"  â€¢ ç¸½éœ€è¦é€£æ¥æ•¸: {total_conns:,}")
    print(f"  â€¢ æ¯ Pod MaxOpenConns: {max_open}")
    print(f"  â€¢ æ¯ Pod MaxIdleConns: {max_idle}")
    print(f"  â€¢ æ¯å€‹ Galera ç¯€é»å¹³å‡é€£æ¥: {total_conns // galera_nodes}")
    
    print(f"\nâ±ï¸  è¶…æ™‚è¨­ç½®:")
    timeout_config = recommend_idle_and_lifetime()
    for key, value in timeout_config.items():
        print(f"  â€¢ {key}: {value}")
    
    print(f"\nğŸ—„ï¸  MariaDB Galera è¨­ç½®:")
    print(f"  â€¢ max_connections: {galera_max_conns}")
    print(f"  â€¢ wsrep_slave_threads: {galera_nodes * 2}")
    
    print(f"\nğŸ’¾ è³‡æºé…ç½® (æ¯å€‹ Pod):")
    for service, config in resources.items():
        print(f"\n  {service.upper()}:")
        for key, value in config.items():
            print(f"    â€¢ {key}: {value}")
    
    # ä¼°ç®—æˆæœ¬ï¼ˆAWSï¼‰
    print(f"\nğŸ’° æˆæœ¬ä¼°ç®— (AWS m5 å¯¦ä¾‹ - ç¾å…ƒ/æœˆ):")
    openfga_cost = scenario.pod_replicas * 50  # m5.large ~$0.096/å°æ™‚
    mariadb_cost = galera_nodes * 150  # m5.2xlarge ~$0.384/å°æ™‚
    storage_cost = 30  # æ¯ 100GB ~$10/æœˆ
    total_cost = openfga_cost + mariadb_cost + storage_cost
    
    print(f"  â€¢ OpenFGA ({scenario.pod_replicas} x m5.large): ${openfga_cost}")
    print(f"  â€¢ MariaDB ({galera_nodes} x m5.2xlarge): ${mariadb_cost}")
    print(f"  â€¢ å­˜å„² (300Gi EBS): ${storage_cost}")
    print(f"  â€¢ ç¸½è¨ˆ: ${total_cost}/æœˆ")
    print(f"  â€¢ æ¯ 1K RPS æˆæœ¬: ${total_cost / (scenario.target_rps / 1000):.2f}")
    
    print("\n" + "="*80)


def generate_yaml_config(
    scenario: RPSScenario,
    galera_nodes: int = 3
) -> str:
    """
    ç”Ÿæˆ Kubernetes YAML é…ç½®ç‰‡æ®µ
    """
    total_conns = calculate_required_connections(scenario)
    max_open, max_idle = calculate_per_pod_config(
        total_conns,
        scenario.pod_replicas,
        galera_nodes
    )
    resources = calculate_cpu_memory_resources(
        scenario.target_rps,
        scenario.avg_latency_ms,
        scenario.pod_replicas
    )
    
    yaml = f"""
# OpenFGA Deployment ç’°å¢ƒè®Šæ•¸é…ç½®
env:
  - name: OPENFGA_DATASTORE_MAX_OPEN_CONNS
    value: "{max_open}"
  
  - name: OPENFGA_DATASTORE_MAX_IDLE_CONNS
    value: "{max_idle}"
  
  - name: OPENFGA_DATASTORE_CONN_MAX_IDLE_TIME
    value: "60s"
  
  - name: OPENFGA_DATASTORE_CONN_MAX_LIFETIME
    value: "10m"

# è³‡æºé…ç½®
resources:
  requests:
    cpu: "{resources['openfga']['cpu_request']}"
    memory: "{resources['openfga']['memory_request']}"
  limits:
    cpu: "{resources['openfga']['cpu_limit']}"
    memory: "{resources['openfga']['memory_limit']}"

# Deployment replicas
replicas: {scenario.pod_replicas}
"""
    
    return yaml.strip()


# é è¨­å ´æ™¯
SCENARIOS = {
    "small": RPSScenario(
        target_rps=1000,
        avg_latency_ms=50,
        safety_factor=1.3,
        pod_replicas=3
    ),
    "medium": RPSScenario(
        target_rps=5000,
        avg_latency_ms=50,
        safety_factor=1.5,
        pod_replicas=5
    ),
    "large": RPSScenario(
        target_rps=10000,
        avg_latency_ms=50,
        safety_factor=1.5,
        pod_replicas=10
    ),
    "xlarge": RPSScenario(
        target_rps=20000,
        avg_latency_ms=50,
        safety_factor=1.5,
        pod_replicas=15
    ),
}


def main():
    """ä¸»å‡½æ•¸"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     OpenFGA + MariaDB Galera é€£æ¥æ± é…ç½®è¨ˆç®—å™¨                               â•‘
â•‘     åŸºæ–¼ 500 è¬ç­†è³‡æ–™å’Œé«˜ RPS è¨­è¨ˆ                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # æ¸¬è©¦å„ç¨®å ´æ™¯
    for name, scenario in SCENARIOS.items():
        print_recommendation(scenario)
        print("\n")
        
        # æ‰“å° YAML é…ç½®
        print(f"ğŸ“ {name.upper()} å ´æ™¯ YAML é…ç½®:")
        print("-" * 80)
        print(generate_yaml_config(scenario))
        print("-" * 80)
        print("\n")


if __name__ == "__main__":
    main()
    
    # äº¤äº’å¼æ¨¡å¼
    print("\nğŸ’¡ äº’å‹•å¼é…ç½®è¨ˆç®—:")
    print("-" * 80)
    
    try:
        target_rps = int(input("ç›®æ¨™ RPS (é»˜èª 10000): ") or "10000")
        avg_latency = int(input("å¹³å‡æŸ¥è©¢å»¶é² ms (é»˜èª 50): ") or "50")
        pod_replicas = int(input("Pod å‰¯æœ¬æ•¸ (é»˜èª 8): ") or "8")
        
        custom_scenario = RPSScenario(
            target_rps=target_rps,
            avg_latency_ms=avg_latency,
            safety_factor=1.5,
            pod_replicas=pod_replicas
        )
        
        print_recommendation(custom_scenario)
        
    except KeyboardInterrupt:
        print("\n\nå·²å–æ¶ˆã€‚")
    except ValueError:
        print("ç„¡æ•ˆçš„è¼¸å…¥ã€‚")
